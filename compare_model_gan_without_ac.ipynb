{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 10000)        42480       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "generation (Dense)              (None, 1)            10001       sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary (Dense)               (None, 10)           100010      sequential_1[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 152,491\n",
      "Trainable params: 152,491\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 100, 100, 16)      448       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 100, 100, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 100, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 16)      4624      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 100, 100, 3)       435       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100, 100, 3)       0         \n",
      "=================================================================\n",
      "Total params: 10,275\n",
      "Trainable params: 10,211\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Combined model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 100, 100, 3)       10275     \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              [(None, 1), (None, 10)]   152491    \n",
      "=================================================================\n",
      "Total params: 162,766\n",
      "Trainable params: 10,211\n",
      "Non-trainable params: 152,555\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 239s 597ms/step\n",
      "Testing for epoch 1:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.15 | 0.8311          | 1.3152\n",
      "generator (test)       | 1.74 | 0.7775          | 0.9591\n",
      "discriminator (train)  | 3.58 | 0.6878          | 2.8953\n",
      "discriminator (test)   | 1.73 | 0.6815          | 1.0478\n",
      "1 time: 0:04:24.929419\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 231s 579ms/step\n",
      "Testing for epoch 2:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.64 | 0.7907          | 0.8497\n",
      "generator (test)       | 1.58 | 0.7558          | 0.8237\n",
      "discriminator (train)  | 2.62 | 0.6962          | 1.9194\n",
      "discriminator (test)   | 1.57 | 0.6875          | 0.8784\n",
      "2 time: 0:04:17.204789\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 230s 576ms/step\n",
      "Testing for epoch 3:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.49 | 0.7701          | 0.7175\n",
      "generator (test)       | 1.48 | 0.7078          | 0.7685\n",
      "discriminator (train)  | 2.27 | 0.7014          | 1.5702\n",
      "discriminator (test)   | 1.50 | 0.6899          | 0.8119\n",
      "3 time: 0:04:15.756882\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 230s 575ms/step\n",
      "Testing for epoch 4:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.41 | 0.7610          | 0.6459\n",
      "generator (test)       | 1.43 | 0.7256          | 0.7063\n",
      "discriminator (train)  | 2.06 | 0.7004          | 1.3553\n",
      "discriminator (test)   | 1.43 | 0.6859          | 0.7480\n",
      "4 time: 0:04:15.653167\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 229s 574ms/step\n",
      "Testing for epoch 5:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.35 | 0.7615          | 0.5901\n",
      "generator (test)       | 1.42 | 0.7378          | 0.6803\n",
      "discriminator (train)  | 1.88 | 0.6991          | 1.1835\n",
      "discriminator (test)   | 1.40 | 0.6881          | 0.7071\n",
      "5 time: 0:04:14.884052\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 230s 574ms/step\n",
      "Testing for epoch 6:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.32 | 0.7727          | 0.5506\n",
      "generator (test)       | 1.34 | 0.7013          | 0.6419\n",
      "discriminator (train)  | 1.74 | 0.6940          | 1.0422\n",
      "discriminator (test)   | 1.36 | 0.6836          | 0.6760\n",
      "6 time: 0:04:15.097330\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 231s 577ms/step\n",
      "Testing for epoch 7:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.32 | 0.7881          | 0.5312\n",
      "generator (test)       | 1.35 | 0.7407          | 0.6088\n",
      "discriminator (train)  | 1.62 | 0.6912          | 0.9307\n",
      "discriminator (test)   | 1.33 | 0.6804          | 0.6497\n",
      "7 time: 0:04:16.952875\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 232s 580ms/step\n",
      "Testing for epoch 8:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.31 | 0.7930          | 0.5122\n",
      "generator (test)       | 1.46 | 0.8511          | 0.6096\n",
      "discriminator (train)  | 1.52 | 0.6953          | 0.8224\n",
      "discriminator (test)   | 1.33 | 0.6925          | 0.6395\n",
      "8 time: 0:04:18.583225\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 232s 580ms/step\n",
      "Testing for epoch 9:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.30 | 0.7980          | 0.4973\n",
      "generator (test)       | 1.37 | 0.7817          | 0.5898\n",
      "discriminator (train)  | 1.43 | 0.6961          | 0.7384\n",
      "discriminator (test)   | 1.31 | 0.6876          | 0.6247\n",
      "9 time: 0:04:18.108138\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 232s 580ms/step\n",
      "Testing for epoch 10:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.29 | 0.7996          | 0.4871\n",
      "generator (test)       | 1.44 | 0.8475          | 0.5966\n",
      "discriminator (train)  | 1.35 | 0.6989          | 0.6498\n",
      "discriminator (test)   | 1.33 | 0.6976          | 0.6307\n",
      "10 time: 0:04:18.591150\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 232s 579ms/step\n",
      "Testing for epoch 11:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.29 | 0.8098          | 0.4844\n",
      "generator (test)       | 1.46 | 0.8721          | 0.5854\n",
      "discriminator (train)  | 1.29 | 0.6990          | 0.5916\n",
      "discriminator (test)   | 1.33 | 0.7043          | 0.6218\n",
      "11 time: 0:04:18.015704\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 231s 578ms/step\n",
      "Testing for epoch 12:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.28 | 0.8088          | 0.4725\n",
      "generator (test)       | 1.38 | 0.7901          | 0.5940\n",
      "discriminator (train)  | 1.24 | 0.7004          | 0.5364\n",
      "discriminator (test)   | 1.32 | 0.6914          | 0.6333\n",
      "12 time: 0:04:16.843977\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 233s 584ms/step\n",
      "Testing for epoch 13:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.27 | 0.8057          | 0.4665\n",
      "generator (test)       | 1.45 | 0.8354          | 0.6137\n",
      "discriminator (train)  | 1.18 | 0.7051          | 0.4732\n",
      "discriminator (test)   | 1.36 | 0.7038          | 0.6544\n",
      "13 time: 0:04:19.067152\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 231s 576ms/step\n",
      "Testing for epoch 14:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.26 | 0.8026          | 0.4617\n",
      "generator (test)       | 1.46 | 0.8313          | 0.6265\n",
      "discriminator (train)  | 1.14 | 0.7072          | 0.4336\n",
      "discriminator (test)   | 1.36 | 0.6999          | 0.6611\n",
      "14 time: 0:04:16.510302\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 230s 576ms/step\n",
      "Testing for epoch 15:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.25 | 0.7964          | 0.4514\n",
      "generator (test)       | 1.43 | 0.8197          | 0.6094\n",
      "discriminator (train)  | 1.10 | 0.7054          | 0.3996\n",
      "discriminator (test)   | 1.36 | 0.7080          | 0.6556\n",
      "15 time: 0:04:15.764981\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 230s 575ms/step\n",
      "Testing for epoch 16:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.24 | 0.7958          | 0.4448\n",
      "generator (test)       | 1.51 | 0.8905          | 0.6212\n",
      "discriminator (train)  | 1.08 | 0.7054          | 0.3714\n",
      "discriminator (test)   | 1.38 | 0.7160          | 0.6650\n",
      "16 time: 0:04:16.160208\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 232s 579ms/step\n",
      "Testing for epoch 17:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.24 | 0.7905          | 0.4486\n",
      "generator (test)       | 1.45 | 0.7884          | 0.6578\n",
      "discriminator (train)  | 1.04 | 0.7057          | 0.3312\n",
      "discriminator (test)   | 1.43 | 0.7053          | 0.7279\n",
      "17 time: 0:04:17.260155\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 230s 576ms/step\n",
      "Testing for epoch 18:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.24 | 0.7942          | 0.4461\n",
      "generator (test)       | 1.45 | 0.8125          | 0.6377\n",
      "discriminator (train)  | 1.02 | 0.7036          | 0.3167\n",
      "discriminator (test)   | 1.38 | 0.6996          | 0.6817\n",
      "18 time: 0:04:16.051637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "400/400 [==============================] - 230s 576ms/step\n",
      "Testing for epoch 19:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.22 | 0.7893          | 0.4287\n",
      "generator (test)       | 1.52 | 0.8484          | 0.6713\n",
      "discriminator (train)  | 0.99 | 0.7002          | 0.2871\n",
      "discriminator (test)   | 1.41 | 0.7084          | 0.7064\n",
      "19 time: 0:04:16.413790\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 231s 577ms/step\n",
      "Testing for epoch 20:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.23 | 0.7926          | 0.4411\n",
      "generator (test)       | 1.44 | 0.7973          | 0.6382\n",
      "discriminator (train)  | 0.97 | 0.7025          | 0.2630\n",
      "discriminator (test)   | 1.39 | 0.7013          | 0.6899\n",
      "20 time: 0:04:16.552899\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the\n",
    "MNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.\n",
    "You should start to see reasonable images after ~5 epochs, and good images\n",
    "by ~15 epochs. You should use a GPU, as the convolution-heavy operations are\n",
    "very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating,\n",
    "as the compilation time can be a blocker using Theano.\n",
    "Timings:\n",
    "Hardware           | Backend | Time / Epoch\n",
    "-------------------------------------------\n",
    " CPU               | TF      | 3 hrs\n",
    " Titan X (maxwell) | TF      | 4 min\n",
    " Titan X (maxwell) | TH      | 7 min\n",
    "Consult https://github.com/lukedeo/keras-acgan for more information and\n",
    "example output\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "#from PIL import Image\n",
    "\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.datasets import mnist\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import  Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.generic_utils import Progbar\n",
    "#from input_data import *\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "num_classes = 10\n",
    "#import numpy as np\n",
    "import os  \n",
    "import cv2\n",
    "import datetime\n",
    "image_row=100\n",
    "image_col=100\n",
    "valid_percent=0.2\n",
    "\n",
    "def load_file_to_list(dir_path,valid_percent):\n",
    "    train_ls=[]\n",
    "    label_ls=[]\n",
    "    \n",
    "    files= os.listdir(dir_path) \n",
    "    for file_label,file in enumerate(files):\n",
    "        if  os.path.isdir(dir_path+file):\n",
    "            loc_path = dir_path+file+'/'\n",
    "            imas = os.listdir(dir_path+file)\n",
    "            for img in imas:\n",
    "                if (not os.path.isdir(img)) and (img!='Thumbs.db'):\n",
    "                    train_ls.append(loc_path+img)\n",
    "                    label_ls.append(file_label)\n",
    "    temp = np.array([train_ls, label_ls])\n",
    "    temp = temp.transpose()\n",
    "    np.random.shuffle(temp)\n",
    "    #label_ls=np.zeros((len(label_ls),len(files)),dtype=\"float32\")\n",
    "    label_ls=np.zeros((len(label_ls),),dtype=\"float32\")\n",
    "    #从打乱的temp中再取出list（img和lab）\n",
    "    #给label手动one-hot\n",
    "    #image_list = list(temp[:, 0])\n",
    "    #label_list = list(temp[:, 1])\n",
    "    #for index,i in enumerate(label_list):        \n",
    "        #label_ls[index,int(i)]=1\n",
    "\n",
    "    #valid_list = image_list[:int(len(image_list)*valid_percent)] \n",
    "    #valid_label = label_ls[:int(len(image_list)*valid_percent),:]\n",
    "    #train_list = image_list[int(len(image_list)*valid_percent):]\n",
    "    #train_label = label_ls[int(len(image_list)*valid_percent):,:]\n",
    "    image_list = list(temp[:, 0])\n",
    "    label_list = list(temp[:, 1])\n",
    "    for index,i in enumerate(label_list):        \n",
    "        label_ls[index]=int(i)\n",
    "\n",
    "\n",
    "    valid_list = image_list[:int(len(image_list)*valid_percent)] \n",
    "    valid_label = label_ls[:int(len(image_list)*valid_percent),]\n",
    "    train_list = image_list[int(len(image_list)*valid_percent):]\n",
    "    train_label = label_ls[int(len(image_list)*valid_percent):,]\n",
    "    return train_list,train_label,valid_list,valid_label\n",
    "\n",
    "def process_line(data_list,data_label,i,scale):\n",
    "    img_x = cv2.imread(data_list[i])\n",
    "    img_y = data_label[i,]\n",
    "    #temp=tf.image.resize_images(img_x, size=[int(image_row/scale),int(image_col/scale)], method=2 )\n",
    "    #img_z = tf.image.resize_images(tf.image.resize_images(img_x, size=[int(image_row/scale),int(image_col/scale)], method=2 )\n",
    "                              # ,size=[image_row,image_col], method=2)\n",
    "    temp = cv2.resize(img_x,None,fx=1/scale,fy=1/scale,interpolation=cv2.INTER_CUBIC)\n",
    "    img_z= cv2.resize(temp,None,fx=scale,fy=scale,interpolation=cv2.INTER_CUBIC)\n",
    "    x = np.array(img_x,dtype=\"float32\")\n",
    "    y = np.array(img_y,dtype=\"float32\")\n",
    "    z = np.array(img_z,dtype=\"float32\")\n",
    "\n",
    "    \n",
    "    return (x[:,:,:]- 127.5) / 127.5,y,(z[:,:,:]- 127.5) / 127.5\n",
    "\n",
    "def batch_train_re(train_list,train_label,batch_size,scale):\n",
    "    X = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "    Y = np.empty((batch_size,),dtype=\"float32\")\n",
    "    Z = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "    cnt=0\n",
    "    i=0\n",
    "    #for i in range(len(train_list)):\n",
    "    while True:\n",
    "        X[cnt,:,:,:],Y[cnt,],Z[cnt,:,:,:]=process_line(train_list,train_label,i,scale)\n",
    "        cnt+=1\n",
    "        if cnt%batch_size==0:\n",
    "            yield X,Y,Z\n",
    "            cnt=0\n",
    "            X = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "            Y = np.empty((batch_size,),dtype=\"float32\")\n",
    "            Z = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "        i += 1\n",
    "        if i>=(len(train_list)):\n",
    "            i =0\n",
    "def batch_valid_re(valid_list,valid_label,batch_size,scale):\n",
    "    X = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "    Y = np.empty((batch_size,),dtype=\"float32\")\n",
    "    Z = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "    cnt=0\n",
    "    i=0\n",
    "    #for i in range(len(train_list)):\n",
    "    while True:\n",
    "        X[cnt,:,:,:],Y[cnt,],Z[cnt,:,:,:]=process_line(valid_list,valid_label,i,scale)\n",
    "        cnt+=1\n",
    "        if cnt%batch_size==0:\n",
    "            yield X,Y,Z\n",
    "            cnt=0\n",
    "            X = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "            Y = np.empty((batch_size,),dtype=\"float32\") \n",
    "            Z = np.empty((batch_size,image_row,image_col,3),dtype=\"float32\")\n",
    "        i += 1\n",
    "        if i>=(len(valid_list)):\n",
    "            i =0\n",
    "def disp_batch_re(train_list,train_label,num_classes,scale):\n",
    "    X = np.empty((num_classes ,image_row,image_col,3),dtype=\"float32\")\n",
    "    Y = np.empty((num_classes ,),dtype=\"float32\")\n",
    "    Z = np.empty((num_classes ,image_row,image_col,3),dtype=\"float32\")   \n",
    "    for i in range(num_classes):\n",
    "        indices = np.argsort(train_label, axis=0)\n",
    "        ind = np.random.randint(0, 100)\n",
    "        k=int(indices[int(i*(len(train_label)/num_classes-1)+ind)])\n",
    "        X[i,:,:,:],Y[i,],Z[i,:,:,:]=process_line(train_list,train_label,k,scale)\n",
    "    return X,Y,Z\n",
    "\n",
    "\n",
    "def build_generator(latent_size):\n",
    "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
    "    # label drawn from P_c, to image space (..., 28, 28, 1)\n",
    "\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(16, kernel_size=3, padding=\"same\",input_shape=(latent_size,latent_size,3)))\n",
    "    cnn.add(Activation(\"relu\"))\n",
    "    cnn.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "    cnn.add(Activation(\"relu\"))\n",
    "    cnn.add(BatchNormalization(momentum=0.8))\n",
    "    cnn.add(Conv2D(16, kernel_size=3, padding=\"same\"))\n",
    "    cnn.add(Activation(\"relu\"))\n",
    "    cnn.add(Conv2D(3, kernel_size=3, padding='same'))\n",
    "    cnn.add(Activation(\"tanh\"))\n",
    "\n",
    "    cnn.summary()\n",
    "    #卧槽，怎么把分类标签信息导入进去\n",
    "    # this is the z space commonly referred to in GAN papers\n",
    "    latent = Input(shape=(latent_size,latent_size,3 ))#输入的低分辨率图像\n",
    "\n",
    "    # this will be our label\n",
    "    #image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    #cls = Reshape((latent_size,latent_size,3))(Embedding(num_classes, latent_size*latent_size*3,\n",
    "                              #embeddings_initializer='glorot_normal')(image_class))\n",
    "    #生成100维的数据，嵌入层\n",
    "    # hadamard product between z-space and a class conditional embedding\n",
    "    #h = layers.multiply([latent, cls])\n",
    "\n",
    "    #fake_image = cnn(h)\n",
    "    fake_image = cnn(latent)\n",
    "\n",
    "    #return Model([latent, image_class], fake_image)\n",
    "    return Model(latent, fake_image)\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
    "    # the reference paper\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Conv2D(32, 3, padding='same', strides=2,\n",
    "                   input_shape=(100,100, 3)))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Conv2D(64, 3, padding='same', strides=1))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Conv2D(32, 3, padding='same', strides=2))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Conv2D(16, 3, padding='same', strides=1))\n",
    "    cnn.add(LeakyReLU(0.2))\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    image = Input(shape=(100, 100, 3))\n",
    "\n",
    "    features = cnn(image)\n",
    "\n",
    "    # first output (name=generation) is whether or not the discriminator\n",
    "    # thinks the image that is being shown is fake, and the second output\n",
    "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
    "    # belongs to.\n",
    "    fake = Dense(1, activation='sigmoid', name='generation')(features)\n",
    "    aux = Dense(num_classes, activation='softmax', name='auxiliary')(features)\n",
    "\n",
    "    return Model(image, [fake, aux])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # batch and latent size taken from the paper\n",
    "    epochs =20\n",
    "    #batch_size = 100\n",
    "    latent_size = 100\n",
    "    scale =2\n",
    "    batch_size=32\n",
    "    dir_path='C:/Users/topchoice/Documents/python_pRa/sar_newBegin/land_more_less/'\n",
    "    train_list,train_label,valid_list,valid_label=load_file_to_list(dir_path,valid_percent)\n",
    "    train_batch=batch_train_re(train_list,train_label,batch_size,scale)\n",
    "    valid_batch=batch_valid_re(valid_list,valid_label,batch_size*100,scale)\n",
    "    # Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
    "    adam_lr = 0.0002\n",
    "    adam_beta_1 = 0.5\n",
    "\n",
    "    # build the discriminator\n",
    "    print('Discriminator model:')\n",
    "    discriminator = build_discriminator()\n",
    "    discriminator.compile(\n",
    "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy'],metrics=['accuracy']\n",
    "    )\n",
    "    discriminator.summary()\n",
    "\n",
    "    # build the generator\n",
    "    generator = build_generator(latent_size)\n",
    "\n",
    "    latent = Input(shape=(latent_size, latent_size,3))\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    # get a fake image\n",
    "    #fake = generator([latent, image_class])\n",
    "    fake = generator(latent)\n",
    "    # we only want to be able to train generation for the combined model\n",
    "    discriminator.trainable = False\n",
    "    fake, aux = discriminator(fake)#输出的fake是D输出的是否为真实图片的概率，aux是D输出的分类向量\n",
    "    combined = Model(latent, [fake, aux])\n",
    "                      #gan_in                  gan_out \n",
    "    print('Combined model:')\n",
    "    combined.compile(\n",
    "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "    )\n",
    "    combined.summary()\n",
    "\n",
    "    # get our mnist data, and force it to be of shape (..., 28, 28, 1) with\n",
    "    # range [-1, 1]\n",
    "\n",
    "\n",
    "    num_train, num_test = len(train_list), len(valid_list)\n",
    "\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs))\n",
    "        start_time = datetime.datetime.now()\n",
    "        num_batches = int(len(train_list) / batch_size)\n",
    "        progress_bar = Progbar(target=num_batches)\n",
    "\n",
    "        # we don't want the discriminator to also maximize the classification\n",
    "        # accuracy of the auxiliary classifier on generated images, so we\n",
    "        # don't train discriminator to produce class labels for generated\n",
    "        # images (see https://openreview.net/forum?id=rJXTf9Bxg).\n",
    "        # To preserve sum of sample weights for the auxiliary classifier,\n",
    "        # we assign sample weight of 2 to the real images.\n",
    "        disc_sample_weight = [np.ones(2 * batch_size),\n",
    "                              np.concatenate((np.ones(batch_size) * 2,\n",
    "                                              np.zeros(batch_size)))]\n",
    "\n",
    "        epoch_gen_loss = []\n",
    "        epoch_disc_loss = []\n",
    "\n",
    "        for index in range(num_batches):\n",
    "            # generate a new batch of noise\n",
    "            #noise = np.random.uniform(-1, 1, (batch_size, latent_size))\n",
    "\n",
    "            # get a batch of real images\n",
    "            image_batch_HR,label_batch ,image_batch_LR = next(train_batch)#修改输入数据，\n",
    "#            image_batch = x_train[index * batch_size:(index + 1) * batch_size]\n",
    "#            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "#\n",
    "#            # sample some labels from p_c\n",
    "#            sampled_labels = np.random.randint(0, num_classes, batch_size)\n",
    "\n",
    "            # generate a batch of fake images, using the generated labels as a\n",
    "            # conditioner. We reshape the sampled labels to be\n",
    "            # (batch_size, 1) so that we can feed them into the embedding\n",
    "            # layer as a length one sequence\n",
    "            #generated_images = generator.predict(\n",
    "                #[image_batch_LR,label_batch.reshape((-1, 1))], verbose=0)\n",
    "            generated_images = generator.predict(\n",
    "                image_batch_LR, verbose=0)\n",
    "            x = np.concatenate((image_batch_HR, generated_images))\n",
    "            \n",
    "            # 可以合并后随机一下\n",
    "            # use one-sided soft real/fake labels\n",
    "            # Salimans et al., 2016\n",
    "            # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4)\n",
    "            soft_zero, soft_one = 0, 0.95\n",
    "            y = np.array([soft_one] * batch_size + [soft_zero] * batch_size)\n",
    "            aux_y = np.concatenate((label_batch, label_batch), axis=0)\n",
    "\n",
    "            # see if the discriminator can figure itself out...\n",
    "            #数据输入的入口，不用怎么改\n",
    "            epoch_disc_loss.append(discriminator.train_on_batch(\n",
    "                x, [y, aux_y], sample_weight=disc_sample_weight))\n",
    "\n",
    "            # make new noise. we generate 2 * batch size here such that we have\n",
    "            # the generator optimize over an identical number of images as the\n",
    "            # discriminator\n",
    "#            noise = np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n",
    "#            sampled_labels = np.random.randint(0, num_classes, 2 * batch_size)\n",
    "            image_batch_HR_2,label_batch_2 ,image_batch_LR_2 = next(train_batch)\n",
    "            x_D=np.concatenate((image_batch_LR,image_batch_LR_2))\n",
    "            sampled_labels_D=np.concatenate((label_batch,label_batch_2))\n",
    "            # we want to train the generator to trick the discriminator\n",
    "            # For the generator, we want all the {fake, not-fake} labels to say\n",
    "            # not-fake\n",
    "            trick = np.ones(2 * batch_size) * soft_one\n",
    "\n",
    "            #epoch_gen_loss.append(combined.train_on_batch(\n",
    "                #[x_D, sampled_labels_D.reshape((-1, 1))],\n",
    "                #[trick, sampled_labels_D]))#主要修改的方向\n",
    "            epoch_gen_loss.append(combined.train_on_batch(\n",
    "                x_D,\n",
    "                [trick, sampled_labels_D]))#主要修改的方向\n",
    "            #combined = Model([latent, image_class], [fake, aux])\n",
    "            progress_bar.update(index + 1)\n",
    "\n",
    "        print('Testing for epoch {}:'.format(epoch))\n",
    "\n",
    "        # evaluate the testing loss here\n",
    "\n",
    "        # generate a new batch of noise\n",
    "        #noise = np.random.uniform(-1, 1, (num_test, latent_size))\n",
    "        image_batch_HR,label_batch ,image_batch_LR = next(valid_batch)\n",
    "        # sample some labels from p_c and generate images from them to test\n",
    "        #sampled_labels = np.random.randint(0, num_classes, num_test)\n",
    "        #generated_images = generator.predict(\n",
    "            #[image_batch_LR, label_batch.reshape((-1, 1))], verbose=False)\n",
    "        generated_images = generator.predict(\n",
    "            image_batch_LR, verbose=False)\n",
    "        \n",
    "        x = np.concatenate((image_batch_HR, generated_images))\n",
    "        y = np.array([1] * batch_size*100  + [0] * batch_size*100 )\n",
    "        aux_y = np.concatenate((label_batch, label_batch), axis=0)\n",
    "\n",
    "        # see if the discriminator can figure itself out...\n",
    "        discriminator_test_loss = discriminator.evaluate(\n",
    "            x, [y, aux_y], verbose=False)\n",
    "\n",
    "        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
    "\n",
    "        # make new noise\n",
    "#        noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n",
    "#        sampled_labels = np.random.randint(0, num_classes, 2 * num_test)\n",
    "        image_batch_HR_2,label_batch_2 ,image_batch_LR_2 = next(valid_batch)\n",
    "        x_G=np.concatenate((image_batch_LR,image_batch_LR_2))\n",
    "        sampled_labels_G=np.concatenate((label_batch,label_batch_2))\n",
    "        \n",
    "        trick = np.ones(2 * num_test)\n",
    "\n",
    "        #generator_test_loss = combined.evaluate(\n",
    "            #[x_G, sampled_labels_G.reshape((-1, 1))],\n",
    "            #[trick, sampled_labels_G], verbose=False)\n",
    "        generator_test_loss = combined.evaluate(\n",
    "            x_G,\n",
    "            [trick, sampled_labels_G], verbose=False)\n",
    "        \n",
    "        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
    "\n",
    "        # generate an epoch report on performance\n",
    "        train_history['generator'].append(generator_train_loss)\n",
    "        train_history['discriminator'].append(discriminator_train_loss)\n",
    "\n",
    "        test_history['generator'].append(generator_test_loss)\n",
    "        test_history['discriminator'].append(discriminator_test_loss)\n",
    "\n",
    "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "            'component', *discriminator.metrics_names))\n",
    "        print('-' * 65)\n",
    "\n",
    "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}'\n",
    "        print(ROW_FMT.format('generator (train)',\n",
    "                             *train_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('generator (test)',\n",
    "                             *test_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (train)',\n",
    "                             *train_history['discriminator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (test)',\n",
    "                             *test_history['discriminator'][-1]))\n",
    "\n",
    "        # save weights every epoch\n",
    "        generator.save_weights(\n",
    "            'ac_gan_diff_chg_weights/params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "        discriminator.save_weights(\n",
    "            'ac_gan_diff_chg_weights/params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"%d time: %s\" % (epoch, elapsed_time))\n",
    "        r, c = 3, 10\n",
    "        #noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        #sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "        image_batch_HR_disp,label_batch_disp ,image_batch_LR_disp=disp_batch_re(train_list,train_label,num_classes,scale)\n",
    "        #gen_imgs = generator.predict([image_batch_LR_disp, label_batch_disp])\n",
    "        gen_imgs = generator.predict(image_batch_LR_disp)\n",
    "    # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        image_batch_HR_disp=0.5*image_batch_HR_disp+0.5\n",
    "        image_batch_LR_disp=0.5*image_batch_LR_disp+0.5\n",
    "        disp_imgs=np.concatenate((image_batch_LR_disp,gen_imgs ,image_batch_HR_disp))\n",
    "        disp_labels=np.concatenate((label_batch_disp ,label_batch_disp,label_batch_disp))\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        fig.suptitle(\"SRACGAN: Generated image\", fontsize=12)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(disp_imgs[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].set_title(\"Cls: %d\" % disp_labels[cnt])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "\n",
    "        fig.savefig(\"./images/SR_withot-ac%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    with open('acgan-history-wiout-ac.pkl', 'wb') as f:\n",
    "        pickle.dump({'train': train_history, 'test': test_history}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
